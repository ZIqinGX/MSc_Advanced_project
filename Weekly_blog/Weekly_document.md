
# week1 Jun12-18

This is the first week of my summer holiday and the beginning of my advanced project. In this week, me and my supervisor Jasper had the first online meeting and talked about my study interest as well as the structure of thesis. The start works including narrowing down research question, thinking about how to collect data, doing some case study.
Jasper also told me some thing need to pay attention to, such as the research question of thesis should be focus and specific.

My research interests lies on combining machine learning as a tool into live performance. There are three missions after class, first,finding gap in knowledge. Second, search for
exsiting technologies (accessible?). Third, think about technical innovation.


# week2 Jun19-25
This online meeting discussed about structure of thesis.
The outline of thesis usually include : 
1 Abstract(to tell people what this paper about); 
2 keywords;
3 Introduction;
4 Literature review;
5 Methodology;
6 Case study;
7 User study/Evaluation;
8 Discussion / Reflection.

I also narrowed my research topic which is how to build a interactive music generation tool? Instrument? which can make audience or composer interact with AI in real time.
This research topic rising is because using AI to assist music creation mainly focuses on generating symbolic music melodies and tunes, and some explore interactive music creation, but there is still a lack of research in live performances where the audience can also involve the sound, so I want to study how to let the audience participate in the music In the interaction of the performance, maybe to generate some harmony? Different melodies?

# week3-5 Jun26-July16
Do not have much progress. Found some papers in related field.
Read the paper called _Performing with a Generative Electronic Music Controller_
Link is here: https://hai-gen.github.io/2022/papers/paper-HAIGEN-MartinCharles.pdf

# Week6 July17-July23

# Week7 July24-July30

**July 25**

Today I found Ryan Kirkbride's wokr, a Python programming environment-- Foxdot and saw his video introducing Live code（Live coding concept）

Link for Foxdot is https://foxdot.org/ 

Link for video is https://www.youtube.com/watch?v=XRNFBZlBeuI

His personal website: https://ryan-kirkbride.github.io/

Ryan Krikbride's paper
https://www.scienceopen.com/hosted-document?doi=10.14236/ewic/eva2015.61

*Collaborative interfaces for ensemble live coding performance* https://etheses.whiterose.ac.uk/28901/ 

Also found some interesting thing:

Genetic Algorithm in python generates music, links are here 

https://www.youtube.com/watch?v=aOsET8KapQQ

https://github.com/kiecodes/generate-music

https://www.youtube.com/watch?v=nypJ3b4rMhE

Using python to create systhesizer

**July 26**
The concept of live code:
> * Using code to describe the rules for music
> * Live notation of composition/notation as performance
> * Unlike traditional code, you can interact with with code while the program is running
> * Take computer programming into social plane

The Algorave music

**July 27**

Today I want to make a personal portfolio website and try this code:<br /> https://www.kaggle.com/code/karnikakapoor/music-generation-lstm/notebook<br />
It is a notebook shows how to use LSTM to generate music.

I found a website can build website by preset modules called squarespace.<br />
linke here  https://www.squarespace.com/ 

# Week8 July30-Aug6
This week, I reproduced a notebook that generates music using LSTM. <br/>The link towards this notebook is https://www.kaggle.com/code/karnikakapoor/music-generation-lstm/notebook#MODEL-BUILDING. It used dataset **Classical Music MIDI**, Source of it is here https://www.kaggle.com/datasets/soumikrakshit/classical-music-midi.
The process is difficult. Chatgpt and Google were used for explaining code and solve error. The system I use is Ubuntu 22.04. I also use GPU to train model. The process of install GPU driver and set correct CUdnn and CUDA is truly difficult. I will have another part in repository to record how I solve the problem I met and the forum where I found answer. <br/>
This week I also have some new inspiration in my interaction design. Here is the background of my starting point and how the design plan forms.<br/>

**BG**  I want to explore the real-time interaction between humans and machine learning models in music performance. For my research project as a graduate student, I aim to achieve a small breakthrough by allowing human-generated variables to influence the music generated by machine learning models. Considering technical complexity, feasibility, and time constraints, there are two possible types of human-generated variables to be considered: one is related to changes in **human actions, such as gestures and body movements**; the other is related to physiological indicators, like **heart rate**. Based on this two possible branch, I have my concept.<br/>
Firstly, I want to start exploring from **gestures**, focusing on how gestures can influence the generated music rhythm. <br/>

**Object**  The object I want to achieve is try to train a model which can capture the change of hand （by camera） and the change of hand with lead to the change of rythm (such as position of hand ,one simple way is read the coordinate of hand. If the position of hand is high, beat per minute is higher, vice versa.)<br/>
Here is a picture can explain my concept:

Secondly, if the first exploration is successful, I will try build connection between music generating model with heart beat change in future.




